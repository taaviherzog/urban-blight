{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884ad444-abfe-4911-b028-c1fe6eedd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3e8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce2d1b",
   "metadata": {},
   "source": [
    "# fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a297357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=8d76ac6b-5ae8-4428-82a4-043130d17b02&limit=32000'\n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "dfFire = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'type', 'desc', 'location', 'date', 'zone_police', 'zone_fire', 'lat', 'lng'\n",
    "dfFire = dfFire.loc[:, ['alarm_time', 'latitude', 'longitude']]\n",
    "\n",
    "# rename columns\n",
    "dfFire.columns = ['date', 'lat', 'lng'] # 'zone' is 'police_zone'\n",
    "\n",
    "# drop nulls\n",
    "dfFire = dfFire.dropna()\n",
    "# just a few nulls, ignoring nulls in 'zone_police' and 'lat' and 'lng'\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "dfFire['date'] = pd.to_datetime(dfFire['date']).apply(lambda x: x.strftime('%Y-%m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26641afb",
   "metadata": {},
   "source": [
    "# crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b8479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    apiURL = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=044f2016-1dfd-4ab0-bc1e-065da05fca2e&limit=320000'\n",
    "    apiURL += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(apiURL)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "dfCrime = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'date', 'location', 'zone', 'desc', 'lng', 'lat'\n",
    "dfCrime = dfCrime.loc[:, ['INCIDENTTIME', 'X', 'Y']]\n",
    "\n",
    "# rename columns\n",
    "dfCrime.columns = ['date', 'lng', 'lat']\n",
    "\n",
    "# drop nulls\n",
    "dfCrime = dfCrime.dropna()\n",
    "# can't do anything without a desc and there were only four more instances of datetime being null so might as well drop\n",
    "# also need non-null 'lat' and 'lng' to reassign 'zone' with knn preds (future step)\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "dfCrime['date'] = pd.to_datetime(dfCrime['date']).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# rearrange 'lng' and 'lat' columns - purely personal aesthetic choice\n",
    "# make the new columns (in the right order)\n",
    "dfCrime['placeholder_lat'] = dfCrime['lat']\n",
    "dfCrime['placeholder_lng'] = dfCrime['lng']\n",
    "# drop the old columns\n",
    "dfCrime = dfCrime.drop(columns = ['lng', 'lat'])\n",
    "# rename the new columns\n",
    "dfCrime = dfCrime.rename(columns = {'placeholder_lat': 'lat', 'placeholder_lng': 'lng'})\n",
    "\n",
    "# ensure 'lat' and 'lng' are float type\n",
    "dfCrime['lat'], dfCrime['lng'] = dfCrime['lat'].astype(float), dfCrime['lng'].astype(float)\n",
    "\n",
    "# drop nulls one final time (I guess because we need to?)\n",
    "dfCrime = dfCrime.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db2eed",
   "metadata": {},
   "source": [
    "# parcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f994d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=adf1fd38-c374-4c4e-9094-5e53bd12419f&limit=32000'\n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "dfParcels = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'parcelID', 'lat', 'lng'\n",
    "dfParcels = dfParcels.loc[:, ['PIN', 'Latitude', 'Longitude']]\n",
    "\n",
    "# rename columns\n",
    "dfParcels.columns = ['parcelID', 'lat', 'lng']\n",
    "\n",
    "# drop nulls - string 'nan' in this case\n",
    "dfParcels = dfParcels.drop(index = [583953, 583954])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214144f",
   "metadata": {},
   "source": [
    "# sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8ee007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=5bbe6c55-bce6-4edb-9d04-68edeb6bf7b1&limit=32000'\n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "df1 = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'parcelID', 'date', 'salePrice', where 'PROPERTYCITY' == 'PITTSBURGH'\n",
    "df1 = df1.loc[df1['PROPERTYCITY'] == 'PITTSBURGH', ['PARID', 'SALEDATE', 'PRICE']]\n",
    "\n",
    "# rename columns\n",
    "df1.columns = ['parcelID', 'date', 'salePrice']\n",
    "\n",
    "# drop nulls\n",
    "df1 = df1.dropna(subset = ['salePrice'])\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "df1['date'] = pd.to_datetime(df1['date']).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=f2b8d575-e256-4718-94ad-1e12239ddb92&limit=32000'\n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "df2 = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'parcelID', 'date', 'salePrice', where 'PROPERTYCITY' == 'PITTSBURGH'\n",
    "df2 = df2.loc[df2['PROPERTYCITY'] == 'PITTSBURGH', ['PARID', 'SALEDATE', 'SALEPRICE']]\n",
    "\n",
    "# rename columns\n",
    "df2.columns = ['parcelID', 'date', 'salePrice']\n",
    "\n",
    "# drop nulls\n",
    "df2 = df2.dropna(subset = ['salePrice'])\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "df2['date'] = pd.to_datetime(df2['date']).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "dfSales = pd.concat([df1, df2], axis = 0).drop_duplicates(subset = ['parcelID', 'date', 'salePrice'])\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "dfSales = dfSales.merge(right = dfParcels, on = 'parcelID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8f474-3d65-4e0c-9ddc-c852c9f8d684",
   "metadata": {},
   "source": [
    "# code violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a16bc2-d1ac-437b-ae84-07ed199dd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=70c06278-92c5-4040-ab28-17671866f81c&limit=32000'\n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "dfViols = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'parcelID', 'date', 'violation'\n",
    "dfViols = dfViols.loc[:, ['parcel_id', 'investigation_date', 'violation_code_section']]\n",
    "\n",
    "# rename columns\n",
    "dfViols.columns = ['parcelID', 'date', 'violation']\n",
    "\n",
    "# drop nulls\n",
    "dfViols = dfViols.dropna()\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "dfViols['date'] = pd.to_datetime(dfViols['date']).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# _____________________________________________________________________________________________________________________________________________________\n",
    "# extra df of historical code violations\n",
    "\n",
    "# dictionary to store temporary dfs\n",
    "dfs = {}\n",
    "\n",
    "# download the data via the provided API\n",
    "offset = 0\n",
    "index = 0\n",
    "while True:\n",
    "    url = 'https://data.wprdc.org/api/3/action/datastore_search?resource_id=4e5374be-1a88-47f7-afee-6a79317019b4&limit=32000'  \n",
    "    url += '&offset=' + str(offset)\n",
    "    fileobj = urllib.request.urlopen(url)\n",
    "    data = pd.DataFrame(json.loads(fileobj.read())['result']['records'])\n",
    "    dfs['df' + str(index)] = data\n",
    "    offset += 32000\n",
    "    index += 1\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "\n",
    "dfViols_historical = pd.concat(dfs.values(), ignore_index = True)\n",
    "\n",
    "# isolate 'date', violation', 'parcelID', 'lng', 'lat'\n",
    "dfViols_historical = dfViols_historical.loc[:, ['INSPECTION_DATE', 'VIOLATION', 'PARCEL', 'X', 'Y']]\n",
    "\n",
    "# rename columns\n",
    "dfViols_historical.columns = ['date', 'violation', 'parcelID', 'lng', 'lat']\n",
    "\n",
    "# drop nulls\n",
    "dfViols_historical = dfViols_historical.dropna()\n",
    "\n",
    "# change 'date' to columns ['month', 'year']\n",
    "dfViols_historical['date'] = pd.to_datetime(dfViols_historical['date']).apply(lambda x: x.strftime('%Y-%m'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16aaa8e",
   "metadata": {},
   "source": [
    "# filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79fe6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = gpd.read_file(\"C:\\\\Users\\\\Taavi\\\\Downloads\\\\Pittsburgh_Boundary.geojson\")\n",
    "\n",
    "boundary = geojson.geometry.iloc[2]\n",
    "# this is the perimeter of the city\n",
    "\n",
    "polygons = [geom for geom in boundary.geoms]\n",
    "    \n",
    "def filterData(data, polygons):\n",
    "    filteredData = []\n",
    "    for index, row in data.iterrows():\n",
    "        dataPoint = Point(row['lng'], row['lat'])\n",
    "        for polygon in polygons:\n",
    "            if dataPoint.within(polygon):\n",
    "                filteredData.append(row)\n",
    "                break\n",
    "    return pd.DataFrame(filteredData)\n",
    "\n",
    "fire = filterData(dfFire, polygons)\n",
    "crime = filterData(dfCrime, polygons)\n",
    "sales = filterData(dfSales, polygons)\n",
    "parcels = filterData(dfParcels, polygons)\n",
    "dfViols = dfViols.merge(right = parcels, on = 'parcelID', how = 'inner')\n",
    "dfViols_historical = dfViols_historical.drop(columns = ['lng', 'lat']).merge(right = parcels, on = 'parcelID', how = 'inner')\n",
    "viols = pd.concat([dfViols, dfViols_historical], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d62657",
   "metadata": {},
   "source": [
    "# reset indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b46f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire = fire.reset_index().drop(columns = 'index')\n",
    "crime = crime.reset_index().drop(columns = 'index')\n",
    "parcels = parcels.reset_index().drop(columns = 'index')\n",
    "sales = sales.reset_index().drop(columns = 'index')\n",
    "viols = viols.reset_index().drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324e7a6",
   "metadata": {},
   "source": [
    "# export csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c50aad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire.to_csv('C:\\\\Users\\\\Taavi\\\\Downloads\\\\rawFire.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0da99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime.to_csv('C:\\\\Users\\\\Taavi\\\\Downloads\\\\rawCrime.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f8568c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.to_csv('C:\\\\Users\\\\Taavi\\\\Downloads\\\\rawParcels.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "087f2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.to_csv('C:\\\\Users\\\\Taavi\\\\Downloads\\\\rawSales.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4dd525-b8bf-46b0-9487-41fa2f0d0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viols.to_csv('C:\\\\Users\\\\Taavi\\\\Downloads\\\\rawViols.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d79c3-2382-4c0d-a013-321be57ffcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
